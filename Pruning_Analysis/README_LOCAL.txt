This file will guide you to use our code to replicate the results of the paper.


This folder contains the following:

Datasets/	  		-- This folder contains the datasets that we have used for the experiments

Output/		  		-- This folder contains the ouput files generated by the scripts 

Script4TestCases/	  	-- This folder contains necessary scripts to run the test cases

TestCases/	 	 	-- This folder contains all the 90 test cases that we have used for the experiments

MainFiles/			-- This folder contains the implementation of verification based testing approach described in the paper


Script_veriTest_prune_analysis_all.py -- Script to run all the test cases to get the results of pruning analysis (RQ 4)

Script_veriTest_prune_short.py	-- Script to run few test cases from all test cases 


##Running experiments of the paper

The next step is to run our experiments. If you want to replicate the results of the pruning analysis on 90 test cases run the following command:

$ python Script_veriTest_prune_analysis_all.py

You can give the option of which type of pruning (strong/weak) you would like to perform and what type of monotonicity you would want to check (strong/weak). Based on your given choice the code will run on 90 test cases.

##Running shorter versions of the experiments

All the experiments for RQ 4 take a significant amount of time to run. Hence, if you do not wish to run the experiments for such long periods of time, we have created scripts for functional testing of our approach.

To get the results of the functional testing for pruning analysis, run the following command on the current directory 


$ python Script_prune_short.py
 

The scripts will generate some intermediate files and datasets. We have removed most of those files. Some of the files will be there which have been generated due to the choice of your pruning strategy.
In the end, the outputs will be stored in the Output folder. The Output file for each algorithm contains the output averaging over all the runs of each test case. For example, kNN*OutputFile contains the result of all the test cases built using kNN algorithm. Execution times or detection rate for each test case have been computed by averaging over all the runs.
 Also, you might see some convergence problems with some ML algorithms.
This is expected.

Our verification based testing approach involves some randomness. We try to minimize this by running all the test cases 10 times and fixing the input parameters. Even after that, it might happen that you encounter a bit inconsistencies with the results mentioned in the paper.

All the execution times written in output files are in seconds.





