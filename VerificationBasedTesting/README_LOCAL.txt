This file will guide you to use our code to replicate the results of the paper.


This folder contains the following:

Datasets/	  		-- This folder contains the datasets that we have used for the experiments

Output/		  		-- This folder contains the ouput files generated by the scripts 

Script4TestCases/	  	-- This folder contains necessary scripts to run the test cases

TestCases/	 	 	-- This folder contains all the 90 test cases that we have used for the experiments

MainFiles/			-- This folder contains the implementation of verification based testing approach described in the paper


Script_veriTest_analysis_all.py -- Script to run all the test cases to get the results of verification based testing (RQ 1, 2, 3)

Script_veriTest_short.py	-- Script to run few test cases from all test cases 


##Running experiments of the paper

The next step is to run our experiments. If you want to replicate the results of the verification based testing approach on 90 test cases run the following command:

$ python Script_veriTest_analysis_all.py


##Running shorter versions of the experiments

All the experiments for RQ 1, 2 and 3 take a significant amount of time to run. Hence, if you do not wish to run the experiments for such long periods of time, we have created scripts for functional testing of our approach.

To get the results of the functional testing for verification based testing approaches, run the following command on the current directory 


$ python Script_veriTest_short.py
 

The scripts will generate some intermediate files and datasets. In the end, the outputs will be stored in the Output folder. The Output file for each algorithm contains the output averaging over all the runs of each test case. For example, kNN*OutputFile contains the result of all the test cases built using kNN algorithm. Execution times or failure attempts for each test case have been computed by averaging over all the runs.
While running 
LightGbm algorithm some deprecation warnings might occur. Also, you might see some convergence problems with some ML algorithms.
This is expected.

Our verification based testing approach involves some randomness. We try to minimize this by running all the test cases 10 times. Even after that, it might happen that you encounter a bit inconsistencies with the results mentioned in the paper.

All the execution times written in output files are in seconds.





